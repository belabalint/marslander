{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.optim import SGD\n",
    "from torch.nn import Linear, Softmax, Sequential, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    Linear(8, 128),\n",
    "    ReLU(),\n",
    "    Linear(128,128),\n",
    "    ReLU(),\n",
    "    Linear(128, 16),\n",
    "    ReLU(),\n",
    "    Linear(16,4),\n",
    "    Softmax(dim=0)  # Ensure softmax is applied along the correct dimension\n",
    ")\n",
    "#inputs: observation tensor\n",
    "#output : Q(s, a1), Q(s, a2), Q(s, a3), Q(s, a4)\n",
    "\n",
    "\n",
    "#this policy chooses from the q table\n",
    "def policy():\n",
    "    return 0\n",
    "\n",
    "#set up hyperparameters\n",
    "discount = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation, info = env.reset(seed=42)\n",
    "optimizer = SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.trange(10000):\n",
    "    recent_batch = []\n",
    "    action = policy(observation, model)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    recent_batch.append({\"observation\":observation, \"action\":action, \"reward\":reward})\n",
    "    \n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
